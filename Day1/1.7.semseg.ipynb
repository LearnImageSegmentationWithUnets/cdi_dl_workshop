{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic segmentation with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Written by Dr Daniel Buscombe, Northern Arizona University\n",
    "\n",
    "> Part of a series of notebooks for image recognition and classification using deep convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates some strategies for semantic image segmentation using common machine learning techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve Bayes classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes models are a group of extremely fast and simple classification algorithms that are often suitable for very high-dimensional datasets. Because they are so fast and have so few tunable parameters, they end up being very useful as a quick-and-dirty baseline for a classification problem. This section will focus on an intuitive explanation of how naive Bayes classifiers work, followed by a couple examples of them in action on some datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection of sand using Naïve Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, Naïve Bayes classification is employed to detect pixels corresponding to sand in images, based just in the pixels color."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data is a M×N×3 array representing a color training image, and mask a M×N binary array representing the classification sand/non-sand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imageio import imread\n",
    "import s3fs\n",
    "fs = s3fs.S3FileSystem(anon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Hoonhout et al 2015](https://www.sciencedirect.com/science/article/pii/S0378383915001313) \"An automated method for semantic classification of regions in coastal images\" Coastal Engineering 105, 1-12\n",
    "\n",
    "Objective: develop a ML classifier for semantic segmentation of images of coasts\n",
    "\n",
    "The full data repository is available [here](https://data.4tu.nl/repository/uuid:08400507-4731-4cb2-a7ec-9ed2937db119). It consists of several images from stationary camera monitoring the Netherlands coast\n",
    "\n",
    "For each image there is an associated 'segments' and 'classes' file. I've just 1 example image and associated classes in the /data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#load classes in\n",
    "infile = 'data/1369558802.Sun.May.26_09_00_02.GMT.2013.jvspeijk.c2.snap.classes.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to read in two files to create a label image\n",
    "These classes are strings of names of features, so we'll recode the class strings ('sky', etc) into numeric codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_classes(infile):\n",
    "    with open(infile, 'rb') as f:\n",
    "        classes = pickle.load(f)\n",
    "    #recast string classes to numeric codes   \n",
    "    codes = np.unique(classes, return_inverse=True)[1].tolist()        \n",
    "    return classes, codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, codes = read_classes(infile)\n",
    "print(len(classes))\n",
    "print(np.unique(classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the 'segments' file there should be 588 image segments (superpixels) each corresponding to the class in the 'classes' vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll load in the 'segments' file. This binary file is encoded a little differently\n",
    "\n",
    "We'll need to make a label image by allocating numeric class codes to each segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_label_image(infile, codes):\n",
    "    with open(infile, 'rb') as f:\n",
    "        u = pickle._Unpickler(f)\n",
    "        u.encoding = 'latin1'\n",
    "        segments = u.load()\n",
    "    \n",
    "    # make label image out of segments\n",
    "    for k in range(len(np.unique(segments))):\n",
    "        segments[segments==k] = codes[k]  \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in image segments   (superpixels)\n",
    "infile = infile.replace('classes', 'segments')\n",
    "segments = make_label_image(infile, codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the segments matrix to see what we're working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(segments)\n",
    "plt.colorbar(shrink=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display that matrix ontop of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in image \n",
    "infile = infile.replace('.segments.pkl', '.jpg')\n",
    "training_rgb = imread(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(segments))\n",
    "print(np.shape(training_rgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there is a problem in the input data set: the label image is smaller than the input image!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_rgb = training_rgb[:1024,:,:]\n",
    "print(np.shape(training_rgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.get_cmap('RdYlBu', 6)    # 6 discrete colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(training_rgb)\n",
    "plt.imshow(segments, alpha=0.5, cmap=cmap)\n",
    "cb = plt.colorbar(shrink=0.5)\n",
    "cb.set_ticks(np.arange(6))\n",
    "cb.set_ticklabels(np.unique(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2 is associated with sand\n",
    "mask = (segments==2).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(training_rgb)\n",
    "plt.title('Input')\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(mask, cmap=plt.cm.binary_r)\n",
    "plt.title('Threshold')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M, N, _ = np.shape(training_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "training_rgb_scaled = preprocessing.scale(training_rgb[:,:,0])\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(training_rgb[:,:,0])\n",
    "plt.title('Input')\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(training_rgb_scaled)\n",
    "plt.title('Standardized')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = training_rgb_scaled.reshape(M*N, -1)[:,:]\n",
    "print(np.shape(data_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification used in the learning step is represented as a binary MN vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = mask.reshape(M*N)\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (fitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn provides a naive_bayes module containing a GaussianNB object that implements the supervised learning by the Gaussian Naïve Bayes method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One extremely fast way to create a simple model is to assume that the data is described by a Gaussian distribution with no covariance between dimensions. This model can be fit by simply finding the mean and standard deviation of the points within each label, which is all you need to define such a distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(data_train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.class_prior_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The priors are just the relative abundances of each class (no sand and sand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sand detection can be performed by reshaping and slicing in the same way as the training image. \n",
    "\n",
    "The predict method of GaussianNB performs the classification. The resulting classification vector can be reshaped to the original image dimensions for visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test at an image from the same place but at a different time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = 'data/1367407802.Wed.May.01_11_30_02.GMT.2013.jvspeijk.c2.snap.jpg' \n",
    "#infile = 'data/1369558802.Sun.May.26_09_00_02.GMT.2013.jvspeijk.c2.snap.jpg'\n",
    "test_rgb = imread(infile)\n",
    "test_rgb = test_rgb[:1024,:,:]\n",
    "test_rgb = preprocessing.scale(test_rgb[:,:,0])\n",
    "M_tst, N_tst = test_rgb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll read in the classes and segments and make a label image like we did before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = infile.replace('snap.jpg', 'snap.classes.pkl')\n",
    "classes, codes = read_classes(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = infile.replace('classes', 'segments')\n",
    "print(infile)\n",
    "mask = make_label_image(infile, codes)==2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = test_rgb.reshape(M_tst * N_tst, -1)[:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run the model we trained on the first image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sand_pred = gnb.predict(data_test)\n",
    "S = sand_pred.reshape(M_tst, N_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(test_rgb)\n",
    "plt.title('New Image')\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(S, cmap=plt.cm.binary_r)\n",
    "plt.title('Model Prediction')\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(mask, cmap=plt.cm.binary_r)\n",
    "plt.title('Target')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also estimate the probabilities of each class because we have a simple recipe to compute the likelihood $P({\\rm features}~|~L_1)$ for any data point, and thus we can quickly compute the posterior ratio and determine which label is the most probable for a given point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sand_pred = gnb.predict_proba(data_test)\n",
    "Sprob = sand_pred.reshape(M_tst, N_tst, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot the probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(Sprob[:,:,1], cmap=plt.cm.bwr, vmax=1)\n",
    "plt.axis('off')\n",
    "plt.colorbar(shrink=0.25)\n",
    "plt.title('Probability of Sand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(test_rgb)\n",
    "plt.title('Test Image')\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,2)\n",
    "#plt.imshow(test_rgb, alpha=0.6)\n",
    "plt.imshow(Sprob[:,:,1]>.3, cmap=plt.cm.binary_r)\n",
    "plt.title('Prob. (Sand) > 0.3')\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(mask, cmap=plt.cm.binary_r)\n",
    "plt.title('Target')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because naive Bayesian classifiers make such stringent assumptions about data, they will generally not perform as well as a more complicated model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we adjust the priors? Say, that there is a 50% prior likelihood of sand pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB(priors=[0.5,0.5])\n",
    "gnb.fit(data_train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sand_pred = gnb.predict_proba(data_test)\n",
    "Sprob = sand_pred.reshape(M_tst, N_tst, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(Sprob[:,:,1], cmap=plt.cm.bwr, vmax=1)\n",
    "plt.axis('off')\n",
    "plt.colorbar(shrink=0.25)\n",
    "plt.title('Probability of Sand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much higher probability of sand, as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(test_rgb)\n",
    "plt.title('Test Image')\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,2)\n",
    "#plt.imshow(test_rgb, alpha=0.6)\n",
    "plt.imshow(Sprob[:,:,1]>.6, cmap=plt.cm.binary_r)\n",
    "plt.title('Prob. (Sand) > 0.6')\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(mask, cmap=plt.cm.binary_r)\n",
    "plt.title('Target')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes with principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make things more complicated by\n",
    "* adding more classes\n",
    "* using feature extraction\n",
    "\n",
    "Unlike before, we can build the feature extraction straight into the model using pipelines, which sequentially apply a list of transforms and a final estimator. In our case we'll use PCA as a transform again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "pca = PCA(svd_solver='randomized', n_components=1, whiten=True, random_state=42) \n",
    "model = make_pipeline(pca, gnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(test_rgb)\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(segments, cmap=plt.cm.binary_r)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = segments.reshape(M*N)\n",
    "model.fit(data_test, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sand_pred = model.predict(data_train)\n",
    "S = sand_pred.reshape(M, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(segments==2)\n",
    "plt.axis('off')\n",
    "plt.title('Sand')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(S==2)\n",
    "plt.axis('off')\n",
    "plt.title('Sand prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, there wasn't much advantage using feature extraction with the NB model. Let's look at a different model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other approaches that attempt to estimate the decision boundaries between different classes. One example is a Gaussian Mixture Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use downscaled versions of images to speed up the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc import imresize\n",
    "\n",
    "infile = 'data/1367407802.Wed.May.01_11_30_02.GMT.2013.jvspeijk.c2.snap.jpg' \n",
    "test_rgb = imread(infile)\n",
    "\n",
    "infile = 'data/1369558802.Sun.May.26_09_00_02.GMT.2013.jvspeijk.c2.snap.jpg'\n",
    "training_rgb = imread(infile)\n",
    "\n",
    "training_rgb = imresize(training_rgb, .125)\n",
    "test_rgb = imresize(test_rgb, .125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(training_rgb)\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(test_rgb)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model with 4 components to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M, N, _ = training_rgb.shape\n",
    "data = training_rgb.reshape(M*N, -1)[:,:]\n",
    "gmm = GaussianMixture(n_components=4, covariance_type=\"tied\").fit(data)\n",
    "labels = gmm.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GMMs use an expectation–maximization approach which qualitatively does the following:\n",
    "\n",
    "Choose starting guesses for the location and shape\n",
    "\n",
    "Repeat until converged:\n",
    "\n",
    "* E-step: for each point, find weights encoding the probability of membership in each cluster\n",
    "* M-step: for each cluster, update its location, normalization, and shape based on all data points, making use of the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll only show every 10th data point to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[::10, 0], data[::10, 1], c=labels[::10], s=5, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply to the test image and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata = test_rgb.reshape(M * N, -1)[:,:]\n",
    "cluster = gmm.predict(newdata)\n",
    "cluster = cluster.reshape(M, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(test_rgb)\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(cluster) #, cmap=plt.cm.binary_r)\n",
    "plt.axis('off')\n",
    "plt.colorbar(shrink=0.1)\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(test_rgb, alpha=0.6)\n",
    "plt.imshow(cluster==3, cmap=plt.cm.binary_r, alpha=0.4)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because GMM contains a probabilistic model under the hood, it is also possible to find probabilistic cluster assignments—in Scikit-Learn this is done using the predict_proba method. This returns a matrix of size [n_samples, n_clusters] which measures the probability that any point belongs to the given cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_probs = gmm.predict_proba(newdata)\n",
    "np.shape(post_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(post_probs[:,3].reshape(M, N), cmap=plt.cm.bwr)\n",
    "plt.axis('off')\n",
    "plt.colorbar(shrink=0.5)\n",
    "plt.title('Probability of Sand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many components?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that GMM is a generative model gives us a natural means of determining the optimal number of components for a given dataset. A generative model is inherently a probability distribution for the dataset, and so we can simply evaluate the likelihood of the data under the model, using cross-validation to avoid over-fitting. \n",
    "\n",
    "Another means of correcting for over-fitting is to adjust the model likelihoods using some analytic criterion such as the Akaike information criterion (AIC) or the Bayesian information criterion (BIC). Scikit-Learn's GMM estimator actually includes built-in methods that compute both of these, and so it is very easy to operate on this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = np.arange(2, 10)\n",
    "models = [GaussianMixture(n, covariance_type='tied', random_state=0).fit(data)\n",
    "          for n in n_components]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(n_components, [m.bic(data) for m in models], 'k--o', label='BIC')\n",
    "plt.plot(n_components, [m.aic(data) for m in models], 'r-s', label='AIC')\n",
    "plt.legend(loc='best')\n",
    "#plt.yscale('log')\n",
    "plt.xlabel('n_components');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, AIC and BIC are virtually the same\n",
    "\n",
    "The optimal number of clusters is the value that minimizes the AIC or BIC. \n",
    "\n",
    "It says about 8 components would have been a better choice than 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=8, covariance_type=\"tied\").fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata = test_rgb.reshape(M * N, -1)[:,:]\n",
    "cluster = gmm.predict(newdata)\n",
    "cluster = cluster.reshape(M, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the means for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gmm.means_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gmm.means_[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_means = np.argsort(np.mean(gmm.means_, axis=1))\n",
    "sorted_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(test_rgb)\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(cluster) \n",
    "plt.axis('off')\n",
    "plt.colorbar(shrink=0.25)\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(test_rgb, alpha=0.6)\n",
    "plt.imshow((cluster==sorted_means[-1])  \n",
    "           + (cluster==sorted_means[-2])\n",
    "           + (cluster==sorted_means[-3]), \n",
    "           cmap=plt.cm.binary_r, alpha=0.4)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like one cluster represents lower beach and another cluster represents upper beach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What choices did we make to arrive at this result?\n",
    "* which cluster corresponds to what feature\n",
    "* number of components\n",
    "* type of covariance\n",
    "\n",
    "How well does this approach generalize to other images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = 'data/1368104406.Thu.May.09_13_00_06.GMT.2013.egmond.c5.snap.jpg'\n",
    "test_rgb2 = imread(infile)\n",
    "\n",
    "test_rgb2 = imresize(test_rgb2, .125)\n",
    "M, N, _ = np.shape(test_rgb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata = test_rgb2.reshape(M * N, -1)[:,:]\n",
    "cluster = gmm.predict(newdata)\n",
    "cluster = cluster.reshape(M, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(test_rgb2)\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(cluster) \n",
    "plt.axis('off')\n",
    "plt.colorbar(shrink=0.25)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Training a classifier with ground truth imagery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the seabright dataset for this task. We have images and associated ground-truth labelled imagery\n",
    "\n",
    "First, we'll load in the images (downsizng them to aid with speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [f for f in fs.ls('cdi-workshop/semseg_data/seabright/train') if f.endswith('.jpg')]\n",
    "len(images)\n",
    "\n",
    "Xtrain = []\n",
    "for file in images:\n",
    "    with fs.open(file, 'rb') as f:\n",
    "        Xtrain.append(imresize(imread(f), .125))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll load the label images, contained in .mat files\n",
    "\n",
    "We need to rescale these data the same way as we rescaled the imagery, but this time we also need to make sure that we keep the label images in whole integeres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "classfiles = [f for f in fs.ls('cdi-workshop/semseg_data/seabright/train/gt') if f.endswith('.mat')]\n",
    "\n",
    "ytrain = []\n",
    "for file in classfiles:\n",
    "    with fs.open(file) as f:\n",
    "        dat = loadmat(f)['class']\n",
    "        datr = np.round(imresize(dat, .125, interp='nearest')/255 * np.max(dat))\n",
    "        ytrain.append(datr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always a good idea to plot some data just to check it is as you expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.subplot(121); plt.imshow(Xtrain[0]); plt.axis('off')\n",
    "plt.subplot(122); plt.imshow(ytrain[0]); plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images, M, N, num_channels = np.shape(Xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, go ahead and fit a Naive Bayes model to the first pair of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using that model, predict the labels for the next image in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a 4-part plot showing \n",
    "1. the original image \n",
    "2. the original label image\n",
    "3. the model-predicted label image\n",
    "4. the pixels associated with water\n",
    "\n",
    "How well does it do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fit a model to the entire dataset, we need to arrange the data a little different\n",
    "\n",
    "The model fitting function expected the image data to be arranged N_features x N_channels (3)\n",
    "\n",
    "and the label data to be \n",
    "\n",
    "N_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for item in Xtrain:\n",
    "    X.append(item.reshape(M*N, -1)[:,:])\n",
    "\n",
    "Xtrain2 = np.vstack(X)\n",
    "\n",
    "Y = []\n",
    "for item in ytrain:\n",
    "    Y.append(item.reshape(M*N))\n",
    "\n",
    "Xtrain2 = np.vstack(X)\n",
    "ytrain2 = np.hstack(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(ytrain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Xtrain2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new model and fit to this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model to an image (you choose) and make a plot as before, exploring how well the model predicts various landcover classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
