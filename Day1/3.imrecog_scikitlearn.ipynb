{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Written by Dr Daniel Buscombe, Northern Arizona University\n",
    "\n",
    "> Part of a series of notebooks for image recognition and classification using deep convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://nbviewer.jupyter.org/github/thsant/scipy4cv/blob/master/07._Scikit-learn.ipynb\n",
    "    \n",
    "https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.07-Support-Vector-Machines.ipynb\n",
    "\n",
    "https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.08-Random-Forests.ipynb\n",
    "\n",
    "https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.09-Principal-Component-Analysis.ipynb\n",
    "\n",
    "https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.11-K-Means.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "fs = s3fs.S3FileSystem(anon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [f for f in fs.ls('cdi-workshop/semseg_data/sandbars') if f.endswith('.JPG')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fs.open('cdi-workshop/imrecog_data/NWPU-RESISC45/test/airplane/airplane_700.jpg', 'rb') as f:\n",
    "    image = color.rgb2gray(imread(f, 'jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = !ls data/RC*.JPG\n",
    "frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.misc import imresize\n",
    "from imageio import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [imresize(imread(f), .125) for f in frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx, ny, nz = np.shape(examples[0])\n",
    "nx, ny, nz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [f.split('/')[-1].split('_')[0] for f in frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=np.arange(len(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(8, 5)\n",
    "fig.set_figheight(15)\n",
    "fig.set_figwidth(15)\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(examples[i])\n",
    "    axi.set(xticks=[], yticks=[],\n",
    "            ylabel=names[target[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [imresize(imread(f), (nx,ny,nz)).reshape(-1, 1) for f in frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {ni: indi for indi, ni in enumerate(set(names))}\n",
    "target = sorted([d[ni] for ni in names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(examples, target,\n",
    "                                                random_state=42, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Xtrain))\n",
    "print(len(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_names = np.unique(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 3)\n",
    "fig.set_figheight(15)\n",
    "fig.set_figwidth(15)\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(Xtest[i].reshape(nx,ny,nz))\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    axi.set_ylabel(uniq_names[ytest[i]],  color= 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=1000)\n",
    "model.fit(np.squeeze(Xtrain), ytrain)\n",
    "ypred = model.predict(np.squeeze(Xtest))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred, ytest\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(ypred, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(shrink=0.75)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.axis('tight')\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(ytest, ypred)\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(mat, classes=uniq_names,\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(mat, classes=uniq_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we hold back some subset of the data from the training of the model, and then use this holdout set to check the model performance\n",
    "\n",
    "a sequence of fits where each subset of the data is used both as a training set and as a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Here we do two validation trials, alternately using each subset of the data as a holdout set. Using the split data from before, we could implement it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_model = model.fit(np.squeeze(Xtrain), ytrain).predict(np.squeeze(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_model = model.fit(np.squeeze(Xtest), ytest).predict(np.squeeze(Xtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(ytrain, y1_model), metrics.accuracy_score(ytest, y2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What comes out are two accuracy scores, which we could combine (by, say, taking the mean) to get a better measure of the global model performance. This particular form of cross-validation is a two-fold cross-validationâ€”that is, one in which we have split the data into two sets and used each in turn as a validation set.\n",
    "\n",
    "We could expand on this idea to use even more trials, and more folds in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split the data into five groups, and use each of them in turn to evaluate the model fit on the other 4/5 of the data. This would be rather tedious to do by hand, and so we can use Scikit-Learn's cross_val_score convenience routine to do it succinctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "X = np.vstack((np.squeeze(Xtrain), np.squeeze(Xtest)))\n",
    "print(np.shape(X))\n",
    "y = np.hstack((ytrain, ytest))\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(model, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating the validation across different subsets of the data gives us an even better idea of the performance of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pca = PCA(svd_solver='randomized', n_components=10, whiten=True, random_state=42)\n",
    "svc = SVC(kernel='rbf', class_weight='balanced')\n",
    "model = make_pipeline(pca, svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'svc__C': [1, 5, 10, 50],\n",
    "              'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}\n",
    "grid = GridSearchCV(model, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time grid.fit(np.squeeze(Xtrain), ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grid.best_estimator_\n",
    "yfit = model.predict(np.squeeze(Xtest))\n",
    "\n",
    "yfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4,4)\n",
    "fig.set_figheight(15)\n",
    "fig.set_figwidth(15)\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(Xtest[i].reshape(nx,ny,nz))\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    axi.set_ylabel(uniq_names[yfit[i]].split()[-1],\n",
    "                   color='black' if yfit[i] == ytest[i] else 'red')\n",
    "fig.suptitle('Predicted Names; Incorrect Labels in Red', size=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(ytest, ypred, target_names=uniq_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(ytest, yfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(mat, classes=uniq_names,\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(mat, classes=uniq_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding airplanes with HOG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using HOG features, we can build up a simple image recognition algorithm with any Scikit-Learn estimator; here we will use a support vector machine.\n",
    "\n",
    "The steps are as follows:\n",
    "\n",
    "* Obtain a set of image thumbnails of airplanes to constitute \"positive\" training samples.\n",
    "* Obtain a set of image thumbnails of non-airplanes to constitute \"negative\" training samples.\n",
    "* Extract HOG features from these training samples.\n",
    "* Train a SVM classifier on these samples.\n",
    "* For an \"unknown\" image, pass a sliding window across the image, using the model to evaluate whether that window contains an airplane or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import feature, color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "fs = s3fs.S3FileSystem(anon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [f for f in fs.ls('cdi-workshop/imrecog_data/NWPU-RESISC45/train/airplane') if f.endswith('.jpg')]\n",
    "\n",
    "positive_patches = []\n",
    "for name in names:\n",
    "    with fs.open(name, 'rb') as f:\n",
    "        positive_patches.append(color.rgb2gray(imread(f, 'jpg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(positive_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 350 airplane images to train on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.ls('cdi-workshop/imrecog_data/NWPU-RESISC45/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names1 = [f for f in fs.ls('cdi-workshop/imrecog_data/NWPU-RESISC45/test/baseball_diamond') if f.endswith('.jpg')]\n",
    "names2 = [f for f in fs.ls('cdi-workshop/imrecog_data/NWPU-RESISC45/test/airport') if f.endswith('.jpg')]\n",
    "names = names1 + names2\n",
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_patches = []\n",
    "for name in names:\n",
    "    with fs.open(name, 'rb') as f:\n",
    "        negative_patches.append(color.rgb2gray(imread(f, 'jpg')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a few of them to get an idea of what they look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 10)\n",
    "fig.set_figheight(15)\n",
    "fig.set_figwidth(15)\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(negative_patches[i], cmap='gray')\n",
    "    axi.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine and extract HOG features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have these positive samples and negative samples, we can combine them and compute HOG features. This step takes a little while, because the HOG features involve a nontrivial computation for each image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "X_train = np.array([feature.hog(im)\n",
    "                    for im in chain(positive_patches,\n",
    "                                    negative_patches)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx, ny = np.shape(X_train)\n",
    "y_train = np.zeros(nx)\n",
    "y_train[:len(positive_patches)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are left with 999 training samples in 72,900 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a support vector machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's use a simple Gaussian naive Bayes to get a quick baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "cross_val_score(GaussianNB(), X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is 60%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For such a high-dimensional binary classification task, a Linear support vector machine is a good choice. We will use Scikit-Learn's LinearSVC, because in comparison to SVC it often has better scaling for large number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the support vector machine, with a grid search over a few choices of the C parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "grid = GridSearchCV(LinearSVC(), {'C': [0.125, 0.25, 0.5, 1.0]})\n",
    "grid.fit(X_train, y_train)\n",
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is 66%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grid.best_estimator_\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding airplanes in new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [f for f in fs.ls('cdi-workshop/imrecog_data/NWPU-RESISC45/test/airplane') if f.endswith('.jpg')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use one image and run a sliding window over it and evaluate each patch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import transform\n",
    "with fs.open(names[20], 'rb') as f:\n",
    "    test_image = imread(f, 'jpg')\n",
    "test_image = color.rgb2gray(test_image)\n",
    "test_image = transform.rescale(test_image,2)\n",
    "plt.imshow(test_image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a window that iterates over patches of this image, and compute HOG features for each patch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(img, patch_size=positive_patches[0].shape,\n",
    "                   istep=2, jstep=2, scale=1.0):\n",
    "    Ni, Nj = (int(scale * s) for s in patch_size)\n",
    "    for i in range(0, img.shape[0] - Ni, istep):\n",
    "        for j in range(0, img.shape[1] - Ni, jstep):\n",
    "            patch = img[i:i + Ni, j:j + Nj]\n",
    "            if scale != 1:\n",
    "                patch = transform.resize(patch, patch_size)\n",
    "            yield (i, j), patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices, patches = zip(*sliding_window(test_image, istep=32, jstep=32))\n",
    "#indices = indices[:100]\n",
    "#patches = patches[:100]\n",
    "len(patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(patches[10], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_hog = np.array([feature.hog(patch) for patch in patches])\n",
    "patches_hog.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can take these HOG-featured patches and use our model to evaluate whether each patch contains an airplane:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = model.predict(patches_hog)\n",
    "labels.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that out of all the patches, we have found a few detections. Let's use the information we have about these patches to show where they lie on our test image, drawing them as rectangles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(test_image, cmap='gray')\n",
    "ax.axis('off')\n",
    "\n",
    "Ni, Nj = positive_patches[0].shape\n",
    "indices = np.array(indices)\n",
    "\n",
    "for i, j in indices[labels == 1]:\n",
    "    ax.add_patch(plt.Rectangle((j, i), Nj, Ni, edgecolor='red',\n",
    "                               alpha=0.3, lw=2, facecolor='none'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view this in a different way by creating a \"heatmap\" of identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = np.zeros(np.shape(test_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in indices[labels == 1]:\n",
    "    heatmap[i:i+Ni, j:j+Nj] = heatmap[i:i+Ni, j:j+Nj]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(10)\n",
    "ax.imshow(test_image, cmap='gray')\n",
    "plt.imshow(heatmap>6, alpha=0.5)\n",
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it on some \"non-airplane\" imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [f for f in fs.ls('cdi-workshop/imrecog_data/NWPU-RESISC45/test/baseball_diamond') if f.endswith('.jpg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fs.open(names[10], 'rb') as f:\n",
    "    test_image = imread(f, 'jpg')\n",
    "test_image = color.rgb2gray(test_image)\n",
    "test_image = transform.rescale(test_image,2)\n",
    "plt.imshow(test_image, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices, patches = zip(*sliding_window(test_image, istep=32, jstep=32))\n",
    "patches_hog = np.array([feature.hog(patch) for patch in patches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = model.predict(patches_hog)\n",
    "labels.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = np.zeros(np.shape(test_image))\n",
    "\n",
    "Ni, Nj = positive_patches[0].shape\n",
    "indices = np.array(indices)\n",
    "\n",
    "for i, j in indices[labels == 1]:\n",
    "    heatmap[i:i+Ni, j:j+Nj] = heatmap[i:i+Ni, j:j+Nj]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(10)\n",
    "ax.imshow(test_image, cmap='gray')\n",
    "plt.imshow(heatmap, alpha=0.5)\n",
    "plt.colorbar()\n",
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
